{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo0930: Insert time series into PostgreSQL/PostGIS and join it with the station info geodata.\n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) as well as weather station information from the DWD Climate Data Center in such a way that it can be used with the **QGIS TimeManager extension**. But this time the **join** of station info geodata and time series are performed in **PostgreSQL/PostGIS** instead of Pandas and CSV file.\n",
    "\n",
    "**UPDATE: The QGIS TimeManager extension is deprecated!** Nevertheless the principle of merging (joining) static location and time varying weather data (temperature, precipitation, etc.) is the same for the **new way to handle time dependent geodata in QGIS.**\n",
    "\n",
    "Below you find the description of how to manage time dependent data with the deprecated TimeManager. This can be transferred to the new QGIS time handling.\n",
    "\n",
    "The TimeManager allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "This relation created by joining station info geodata with time series is a 1:N relationship: 1 station has N measurements values. They can be distinguished by timestamp. Technically the primary key for that relation consists of the two attributes (station_id, timestamp). \n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "Primary key of this example relation is (station_id, meas_time).\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "This relation was realized in an earlier activity in Pandas and saved as CSV which then was imported to QGIS and used in the TimeManager. This approach is quite brute force, because the data is highly redundent. Example: If the time series at a single station X contains 1000 values then the feature table will contain 1000 rows for that station, one feature with geometry information and measurement value for each timestamp of the time series. Neither station id, station name nor coordinates differ. The only difference are the timestamps and the associated measurement values. And all these 1000 features belonging to one station are plotted on top of each other. The TimeManager then selects from the feature table only those features which match a given timestamp. In this selection each station occurs only once. This view is a snapshot of the precipitation measurements at all stations included for a given time.\n",
    "\n",
    "This activity demonstrates an alternative approach. Instead of writing the 1:N relationship to a CSV file (which can become very large!) and importing this to QGIS the join is performed in PostGIS. The two relations (tables) involved are the station info layer with geometry column (primary key: station_id) and the table with the precipitation time series (Promary key: station_id, timestamp). The join of these tables is then stored as a view. This is a kind of virtual table. When you select from the view it looks as it where a table (in fact, it is a relation), but the information is selected and joined from the underlying tables during execution time.\n",
    "\n",
    "This stored view can be imported in QGIS as point vector layer as if it were a geodata table. It is noteworthy that this link is live connection. Any change of the data in PostGIS will be immediately updated in QGIS and vice versa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection\n",
    "\n",
    "* FTP: ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/\n",
    "* HTTPS: https://opendata.dwd.de/climate_environment/CDC/observations_germany/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/recent/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only Stations Located in NRW and being Operational "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\")].index\n",
    "#station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas - Create a Geo Data Frame\n",
    "\n",
    "A Geopandas geo data frame is a Pandas data frame enriched with an additional geometry column. Each row in the data frame becomes a location information. Thus a geo-df contains geometry and attributes, i.e. full features. The geo-df is self-contained and complete. It can be easily saved in different vectore file formats, i.e. shapefile or geopackage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: some `pyproj` installations with wrong `PROJ_LIB` environment variable value \n",
    "\n",
    "Problem:\n",
    "\n",
    "```\n",
    "C:\\Users\\me\\Anaconda3\\envs\\geo\\lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n",
    "  _pyproj_global_context_initialize()\n",
    "[...]\n",
    "CRSError: Invalid projection: epsg:4326: (Internal Proj Error: proj_create: no database context specified)\n",
    "```\n",
    "\n",
    "\n",
    "This problem seems to occur on Windows when using the OSGeo4W installer. The environment variable must point to a user specific directory and according to the activated conda environment, e.g. `PROJ_LIB=C:\\Users\\<username>\\Anaconda3\\envs\\geo\\Library\\share\\proj` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct wrong environment variable value occurring when using OSGeo4W installer\n",
    "\n",
    "import os\n",
    "#proj_lib = os.environ['proj_lib']\n",
    "#print(proj_lib)\n",
    "#-> C:\\OSGeo4W64\\share\\proj (wrong!)\n",
    "\n",
    "conda_prefix = os.environ['conda_prefix']\n",
    "print(f\"CONDA_PREFIX: {conda_prefix:s}\")\n",
    "os.environ['proj_lib'] = conda_prefix + r\"\\Library\\share\\proj\"\n",
    "proj_lib = os.environ['proj_lib']\n",
    "print(f\"New env var value: \\nPROJ_LIB={proj_lib:s}\")\n",
    "#-> C:\\Users\\me\\Anaconda3\\envs\\geo\\Library\\share\\proj (correct!)\n",
    "\n",
    "# Now pyproj should work\n",
    "import pyproj\n",
    "print(f\"pyproj.datadir.get_data_dir() -> {pyproj.datadir.get_data_dir():s}\") \n",
    "\n",
    "# Now geopandas (it uses pyproj) should work:\n",
    "# import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgresql://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Geopandas Data Frame directly into PostGIS Database\n",
    "\n",
    "* https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "* https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "* https://www.postgresqltutorial.com/postgresql-primary-key/\n",
    "* https://www.postgresql.org/docs/13/sql-altertable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "# https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "#import psycopg2\n",
    "\n",
    "engine = create_engine(db_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types in PG explicitly.\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\" : Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "\n",
    "# to_postgis() option if_exists = \"replace\" does not imply cascade. The table cannot be replaced if other objects depend on it, i.e. the view used later. \n",
    "# This raises an error.\n",
    "# you may have to drop the table explicitly with option cascade or drop the view.\n",
    "# ATTENTION! This drops all dependent objects, too, i.e. the view!\n",
    "#engine.execute('drop table dwd.stations cascade') \n",
    "# Just drop the view ...\n",
    "engine.execute(\"DROP VIEW IF EXISTS dwd.v_stations_prec\")\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "#engine.execute('alter table dwd.stations add constraint my_awesome_pkey primary key (station_id)')\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Time Series Zip Archives\n",
    "\n",
    "Extract the product file (txt file containing several time series for different variables) from an archive, extract the relevant time series from the product file, limit the time series interval if needed and append it to a dataframe. Finally insert the dataframe to the PostGIS database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files from FTP Directory Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "**Problem:** Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "# SHORTENED FOR TESTING!\n",
    "#station_ids_selected = list(dfNRW.index)[:2]\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_zip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Write the time series to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from my_dwd import prec_ts_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code is not necessary! It just produces a large CSV file in your folder for testing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CODE EXECUTION DEACTIVATED! Change it to True if you want to run it.\n",
    "#if True:\n",
    "if False:\n",
    "    # Produce CSV with sequentially appended time series, ca. 120 MB!\n",
    "    csvfname = \"prec_ts_appended_3_cols.csv\"\n",
    "    first = False\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "                # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "                dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "                dftmp.rename_axis('ts', inplace = True)\n",
    "                # dftmp.to_csv(f, header=f.tell()==0)\n",
    "                if (first):\n",
    "                    first = False\n",
    "                    dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "                else:\n",
    "                    dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "\n",
    "    dftmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The database writer (SQL)! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create View joining static station info with the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute( \\\n",
    "\"\"\"\n",
    "CREATE OR REPLACE VIEW dwd.v_stations_prec \n",
    "as (select t1.station_id, t2.ts, t2.val, t1.geometry \n",
    "from dwd.stations t1, dwd.prec t2 \n",
    "where t2.ts between '2021-07-14T00:00:00UTC' and '2021-07-15T00:00:00UTC'\n",
    "and t1.station_id = t2.station_id)\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some checks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://geo_master:xxxxxx@localhost/geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select max(ts) from dwd.v_stations_prec \n",
    "#select * from dwd.v_stations_prec where ts between '2022-02-15 23:00:00+01:00' and '2022-02-16 00:00:00+01:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from dwd.v_stations_prec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Python modules are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
