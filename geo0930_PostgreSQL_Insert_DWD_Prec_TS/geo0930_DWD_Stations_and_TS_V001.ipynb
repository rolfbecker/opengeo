{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo0930: Insert time series into PostgreSQL/PostGIS and join it with the station info geodata.\n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) as well as weather station information from the DWD Climate Data Center in such a way that it can be used with the **QGIS TimeManager extension**. But this time the **join** of station info geodata and time series are performed in **PostgreSQL/PostGIS** instead of Pandas and CSV file.\n",
    "\n",
    "The TimeManager allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "This relation created by joining station info geodata with time series is a 1:N relationship: 1 station has N measurements values. They can be distinguished by timestamp. Technically the primary key for that relation consists of the two attributes (station_id, timestamp). \n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "Primary key of this example relation is (station_id, meas_time).\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "This relation was realized in an earlier activity in Pandas and saved as CSV which then was imported to QGIS and used in the TimeManager. This approach is quite brute force, because the data is highly redundent. Example: If the time series at a single station X contains 1000 values then the feature table will contain 1000 rows for that station, one feature with geometry information and measurement value for each timestamp of the time series. Neither station id, station name nor coordinates differ. The only difference are the timestamps and the associated measurement values. And all these 1000 features belonging to one station are plotted on top of each other. The TimeManager then selects from the feature table only those features which match a given timestamp. In this selection each station occurs only once. This view is a snapshot of the precipitation measurements at all stations included for a given time.\n",
    "\n",
    "This activity demonstrates an alternative approach. Instead of writing the 1:N relationship to a CSV file (which can become very large!) and importing this to QGIS the join is performed in PostGIS. The two relations (tables) involved are the station info layer with geometry column (primary key: station_id) and the table with the precipitation time series (Promary key: station_id, timestamp). The join of these tables is then stored as a view. This is a kind of virtual table. When you select from the view it looks as it where a table (in fact, it is a relation), but the information is selected and joined from the underlying tables during execution time.\n",
    "\n",
    "This stored view can be imported in QGIS as point vector layer as if it were a geodata table. It is noteworthy that this link is live connection. Any change of the data in PostGIS will be immediately updated in QGIS and vice versa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection\n",
    "\n",
    "* FTP: ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/\n",
    "* HTTPS: https://opendata.dwd.de/climate_environment/CDC/observations_germany/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/recent/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/original/DWD/\n",
      "../data/original/DWD//hourly/precipitation/recent/\n",
      "../data/original/DWD//hourly/precipitation/recent/\n",
      "\n",
      "../data/generated/DWD/\n",
      "../data/generated/DWD//hourly/precipitation/recent/\n",
      "../data/generated/DWD//hourly/precipitation/recent/\n",
      "../data/generated/DWD//hourly/precipitation/recent/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_hourly_precipi...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>68888</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_hourly_precipit...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>68313</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>RR_Stundenwerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>209079</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>stundenwerte_RR_00020_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>43962</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>stundenwerte_RR_00044_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>44254</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_hourly_precipi...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_hourly_precipit...  .pdf   \n",
       "2          -1         RR_Stundenwerte_Beschreibung_Stationen.txt  .txt   \n",
       "3          20                      stundenwerte_RR_00020_akt.zip  .zip   \n",
       "4          44                      stundenwerte_RR_00044_akt.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0   68888    -  \n",
       "1   68313    -  \n",
       "2  209079    -  \n",
       "3   43962    -  \n",
       "4   44254    -  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station description file name:\n",
      "RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile(ftp, src, dest):\n",
      "FTP source: /climate_environment/CDC/observations_germany/climate//hourly/precipitation/recent/RR_Stundenwerte_Beschreibung_Stationen.txt\n",
      "Local dest:   ../data/original/DWD//hourly/precipitation/recent/RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>432</td>\n",
       "      <td>48.9220</td>\n",
       "      <td>9.9129</td>\n",
       "      <td>Abtsgmünd-Untergröningen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2005-10-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>60</td>\n",
       "      <td>52.5850</td>\n",
       "      <td>13.5634</td>\n",
       "      <td>Ahrensfelde</td>\n",
       "      <td>Brandenburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2004-10-22</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>759</td>\n",
       "      <td>48.2156</td>\n",
       "      <td>8.9784</td>\n",
       "      <td>Albstadt-Badkap</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1995-09-01 2011-04-01       202   50.7827     6.0941   \n",
       "20         2004-08-14 2021-02-11       432   48.9220     9.9129   \n",
       "44         2007-04-01 2021-02-11        44   52.9336     8.2370   \n",
       "53         2005-10-01 2021-02-11        60   52.5850    13.5634   \n",
       "71         2004-10-22 2020-01-01       759   48.2156     8.9784   \n",
       "\n",
       "                                name                state  \n",
       "station_id                                                 \n",
       "3                             Aachen  Nordrhein-Westfalen  \n",
       "20          Abtsgmünd-Untergröningen    Baden-Württemberg  \n",
       "44                      Großenkneten        Niedersachsen  \n",
       "53                       Ahrensfelde          Brandenburg  \n",
       "71                   Albstadt-Badkap    Baden-Württemberg  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only Stations Located in NRW and being Operational "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\")].index\n",
    "#station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13671</th>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>221</td>\n",
       "      <td>50.9655</td>\n",
       "      <td>7.2753</td>\n",
       "      <td>Overath-Böke</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13696</th>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>60</td>\n",
       "      <td>51.5966</td>\n",
       "      <td>7.4048</td>\n",
       "      <td>Waltrop-Abdinghof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>2008-05-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>205</td>\n",
       "      <td>51.3329</td>\n",
       "      <td>7.3411</td>\n",
       "      <td>Gevelsberg-Oberbröking</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>2007-11-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>386</td>\n",
       "      <td>51.0899</td>\n",
       "      <td>7.6289</td>\n",
       "      <td>Meinerzhagen-Redlendorf</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>231</td>\n",
       "      <td>50.7983</td>\n",
       "      <td>6.0244</td>\n",
       "      <td>Aachen-Orsbach</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2021-02-11       298   51.1143     7.8807   \n",
       "389        2009-11-01 2021-02-11       436   51.0148     8.4318   \n",
       "390        2004-07-01 2021-02-11       610   50.9837     8.3683   \n",
       "554        1995-09-01 2021-02-11        23   51.8293     6.5365   \n",
       "603        1999-03-03 2021-02-11       147   50.7293     7.2040   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "13671      2007-12-01 2021-02-11       221   50.9655     7.2753   \n",
       "13696      2007-12-01 2021-02-11        60   51.5966     7.4048   \n",
       "13700      2008-05-01 2021-02-11       205   51.3329     7.3411   \n",
       "13713      2007-11-01 2021-02-11       386   51.0899     7.6289   \n",
       "15000      2011-04-01 2021-02-11       231   50.7983     6.0244   \n",
       "\n",
       "                                    name                state  \n",
       "station_id                                                     \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen  \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen  \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen  \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "...                                  ...                  ...  \n",
       "13671                       Overath-Böke  Nordrhein-Westfalen  \n",
       "13696                  Waltrop-Abdinghof  Nordrhein-Westfalen  \n",
       "13700             Gevelsberg-Oberbröking  Nordrhein-Westfalen  \n",
       "13713            Meinerzhagen-Redlendorf  Nordrhein-Westfalen  \n",
       "15000                     Aachen-Orsbach  Nordrhein-Westfalen  \n",
       "\n",
       "[81 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas - Create a Geo Data Frame\n",
    "\n",
    "A Geopandas geo data frame is a Pandas data frame enriched with an additional geometry column. Each row in the data frame becomes a location information. Thus a geo-df contains geometry and attributes, i.e. full features. The geo-df is self-contained and complete. It can be easily saved in different vectore file formats, i.e. shapefile or geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.88070 51.11430)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.43180 51.01480)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.36830 50.98370)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (6.53650 51.82930)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.20400 50.72930)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2021-02-11       298   51.1143     7.8807   \n",
       "389        2009-11-01 2021-02-11       436   51.0148     8.4318   \n",
       "390        2004-07-01 2021-02-11       610   50.9837     8.3683   \n",
       "554        1995-09-01 2021-02-11        23   51.8293     6.5365   \n",
       "603        1999-03-03 2021-02-11       147   50.7293     7.2040   \n",
       "\n",
       "                                    name                state  \\\n",
       "station_id                                                      \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen   \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen   \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen   \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen   \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen   \n",
       "\n",
       "                            geometry  \n",
       "station_id                            \n",
       "216         POINT (7.88070 51.11430)  \n",
       "389         POINT (8.43180 51.01480)  \n",
       "390         POINT (8.36830 50.98370)  \n",
       "554         POINT (6.53650 51.82930)  \n",
       "603         POINT (7.20400 50.72930)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection URL:  postgres://geo_master:xxxxxx@localhost:5432/geo\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgres://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Geopandas Data Frame directly into PostGIS Database\n",
    "\n",
    "* https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "* https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "* https://www.postgresqltutorial.com/postgresql-primary-key/\n",
    "* https://www.postgresql.org/docs/13/sql-altertable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x14f99e7c520>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "# https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "\n",
    "engine = create_engine(db_connection_url)\n",
    "\n",
    "# Set data types in PG explicitly.\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\" : Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "#engine.execute('alter table dwd.stations add constraint my_awesome_pkey primary key (station_id)')\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Time Series Zip Archives\n",
    "\n",
    "Extract the product file (txt file containing several time series for different variables) from an archive, extract the relevant time series from the product file, limit the time series interval if needed and append it to a dataframe. Finally insert the dataframe to the PostGIS database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files from FTP Directory Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stundenwerte_RR_00020_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>43962</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>stundenwerte_RR_00044_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>44254</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stundenwerte_RR_00053_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>42420</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stundenwerte_RR_00071_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>14415</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>stundenwerte_RR_00073_akt.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>43449</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name   ext   size type\n",
       "station_id                                                 \n",
       "20          stundenwerte_RR_00020_akt.zip  .zip  43962    -\n",
       "44          stundenwerte_RR_00044_akt.zip  .zip  44254    -\n",
       "53          stundenwerte_RR_00053_akt.zip  .zip  42420    -\n",
       "71          stundenwerte_RR_00071_akt.zip  .zip  14415    -\n",
       "73          stundenwerte_RR_00073_akt.zip  .zip  43449    -"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "**Problem:** Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stundenwerte_RR_00216_akt.zip\n",
      "stundenwerte_RR_00389_akt.zip\n",
      "stundenwerte_RR_00390_akt.zip\n",
      "stundenwerte_RR_00554_akt.zip\n",
      "stundenwerte_RR_00603_akt.zip\n",
      "stundenwerte_RR_00613_akt.zip\n",
      "stundenwerte_RR_00617_akt.zip\n",
      "stundenwerte_RR_00644_akt.zip\n",
      "stundenwerte_RR_00796_akt.zip\n",
      "stundenwerte_RR_00871_akt.zip\n",
      "stundenwerte_RR_00902_akt.zip\n",
      "stundenwerte_RR_00934_akt.zip\n",
      "stundenwerte_RR_00989_akt.zip\n",
      "stundenwerte_RR_01024_akt.zip\n",
      "stundenwerte_RR_01046_akt.zip\n",
      "stundenwerte_RR_01078_akt.zip\n",
      "stundenwerte_RR_01241_akt.zip\n",
      "stundenwerte_RR_01246_akt.zip\n",
      "stundenwerte_RR_01300_akt.zip\n",
      "stundenwerte_RR_01303_akt.zip\n",
      "stundenwerte_RR_01327_akt.zip\n",
      "stundenwerte_RR_01590_akt.zip\n",
      "stundenwerte_RR_01595_akt.zip\n",
      "stundenwerte_RR_01766_akt.zip\n",
      "stundenwerte_RR_02027_akt.zip\n",
      "stundenwerte_RR_02110_akt.zip\n",
      "stundenwerte_RR_02473_akt.zip\n",
      "stundenwerte_RR_02483_akt.zip\n",
      "stundenwerte_RR_02497_akt.zip\n",
      "stundenwerte_RR_02629_akt.zip\n",
      "stundenwerte_RR_02667_akt.zip\n",
      "stundenwerte_RR_02810_akt.zip\n",
      "stundenwerte_RR_02947_akt.zip\n",
      "stundenwerte_RR_02968_akt.zip\n",
      "stundenwerte_RR_02999_akt.zip\n",
      "stundenwerte_RR_03028_akt.zip\n",
      "stundenwerte_RR_03031_akt.zip\n",
      "stundenwerte_RR_03081_akt.zip\n",
      "stundenwerte_RR_03098_akt.zip\n",
      "stundenwerte_RR_03215_akt.zip\n",
      "stundenwerte_RR_03321_akt.zip\n",
      "stundenwerte_RR_03339_akt.zip\n",
      "stundenwerte_RR_03499_akt.zip\n",
      "stundenwerte_RR_03540_akt.zip\n",
      "stundenwerte_RR_03591_akt.zip\n",
      "stundenwerte_RR_03795_akt.zip\n",
      "stundenwerte_RR_03913_akt.zip\n",
      "stundenwerte_RR_04063_akt.zip\n",
      "stundenwerte_RR_04127_akt.zip\n",
      "stundenwerte_RR_04150_akt.zip\n",
      "stundenwerte_RR_04313_akt.zip\n",
      "stundenwerte_RR_04368_akt.zip\n",
      "stundenwerte_RR_04371_akt.zip\n",
      "stundenwerte_RR_04400_akt.zip\n",
      "stundenwerte_RR_04488_akt.zip\n",
      "stundenwerte_RR_04741_akt.zip\n",
      "stundenwerte_RR_04849_akt.zip\n",
      "stundenwerte_RR_05064_akt.zip\n",
      "stundenwerte_RR_05360_akt.zip\n",
      "stundenwerte_RR_05480_akt.zip\n",
      "stundenwerte_RR_05513_akt.zip\n",
      "stundenwerte_RR_05619_akt.zip\n",
      "stundenwerte_RR_05699_akt.zip\n",
      "stundenwerte_RR_05717_akt.zip\n",
      "stundenwerte_RR_05733_akt.zip\n",
      "stundenwerte_RR_06197_akt.zip\n",
      "stundenwerte_RR_06264_akt.zip\n",
      "stundenwerte_RR_06313_akt.zip\n",
      "stundenwerte_RR_06337_akt.zip\n",
      "stundenwerte_RR_07106_akt.zip\n",
      "stundenwerte_RR_07330_akt.zip\n",
      "stundenwerte_RR_07344_akt.zip\n",
      "stundenwerte_RR_07374_akt.zip\n",
      "stundenwerte_RR_07378_akt.zip\n",
      "stundenwerte_RR_13669_akt.zip\n",
      "stundenwerte_RR_13670_akt.zip\n",
      "stundenwerte_RR_13671_akt.zip\n",
      "stundenwerte_RR_13696_akt.zip\n",
      "stundenwerte_RR_13700_akt.zip\n",
      "stundenwerte_RR_13713_akt.zip\n",
      "stundenwerte_RR_15000_akt.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_zip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from my_dwd import prec_ts_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00216_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00216.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00389_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00389.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00390_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00390.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00554_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00554.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00603_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00603.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00613_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00613.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00617_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00617.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00644_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00644.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00796_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00796.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00871_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00871.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00902_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00902.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00934_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00934.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_00989_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_00989.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01024_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01024.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01046_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01046.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01078_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01078.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01241_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01241.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01246_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01246.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01300_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01300.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01303_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01303.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01327_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01327.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01590_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01590.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01595_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01595.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_01766_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_01766.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02027_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02027.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02110_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02110.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02473_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02473.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02483_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02483.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02497_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02497.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02629_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02629.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02667_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02667.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02810_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02810.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02947_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02947.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02968_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02968.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_02999_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_02999.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03028_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03028.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03031_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03031.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03081_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03081.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03098_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03098.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03215_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03215.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03321_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03321.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03339_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03339.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03499_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03499.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03540_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03540.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03591_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03591.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03795_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03795.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_03913_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_03913.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04063_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04063.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04127_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04127.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04150_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04150.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04313_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04313.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04368_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04368.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04371_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04371.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04400_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04400.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04488_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04488.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04741_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04741.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_04849_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_04849.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05064_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05064.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05360_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05360.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05480_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05480.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05513_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05513.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05619_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05619.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05699_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05699.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05717_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05717.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_05733_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_05733.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_06197_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_06197.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_06264_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_06264.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_06313_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_06313.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_06337_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_06337.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_07106_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_07106.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_07330_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_07330.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_07344_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_07344.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_07374_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_07374.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_07378_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_07378.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_13669_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_13669.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_13670_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_13670.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_13671_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_13671.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_13696_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_13696.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_13713_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_13713.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/recent/stundenwerte_RR_15000_akt.zip\n",
      "Extract product file: produkt_rr_stunde_20190810_20210209_15000.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csvfname = \"prec_ts_appended_3_cols.csv\"\n",
    "\n",
    "first = False\n",
    "\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "            else:\n",
    "                dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stations_id</th>\n",
       "      <th>r1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mess_datum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-07 00:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07 01:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07 02:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07 03:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-07 04:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06 19:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06 20:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06 21:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06 22:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06 23:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           stations_id   r1\n",
       "mess_datum                                 \n",
       "2019-08-07 00:00:00+00:00        15000  0.0\n",
       "2019-08-07 01:00:00+00:00        15000  0.0\n",
       "2019-08-07 02:00:00+00:00        15000  0.0\n",
       "2019-08-07 03:00:00+00:00        15000  0.0\n",
       "2019-08-07 04:00:00+00:00        15000  0.0\n",
       "...                                ...  ...\n",
       "2021-02-06 19:00:00+00:00        15000  3.2\n",
       "2021-02-06 20:00:00+00:00        15000  2.6\n",
       "2021-02-06 21:00:00+00:00        15000  0.9\n",
       "2021-02-06 22:00:00+00:00        15000  0.6\n",
       "2021-02-06 23:00:00+00:00        15000  0.8\n",
       "\n",
       "[13200 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract product file: produkt_rr_stunde_20190811_20210210_00216.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00389.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00390.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00554.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00603.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00613.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00617.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00644.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00796.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00871.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00902.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00934.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_00989.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01024.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01046.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01078.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01241.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01246.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01300.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01303.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01327.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01590.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01595.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_01766.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02027.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02110.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02473.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02483.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02497.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02629.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02667.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02810.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02947.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02968.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_02999.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03028.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03031.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03081.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03098.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03215.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03321.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03339.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03499.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03540.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03591.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03795.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_03913.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04063.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04127.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04150.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04313.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04368.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04371.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04400.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04488.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04741.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_04849.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05064.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05360.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05480.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05513.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05619.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05699.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05717.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_05733.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_06197.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_06264.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_06313.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_06337.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_07106.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_07330.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_07344.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_07374.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_07378.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13669.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13670.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13671.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13696.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13700.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_13713.txt\n",
      "Extract product file: produkt_rr_stunde_20190811_20210210_15000.txt\n",
      "create index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x14f9a5bbd60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2e3c0d182e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"CREATE OR REPLACE VIEW dwd.v_stations_prec as (select t1.station_id, t2.ts, t2.val, t1.geometry from dwd.stations t1, dwd.prec t2 where t1.station_id = t2.station_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['builtins', 'builtins', 'os', 'ftplib', 'pandas', 'fiona', 'types']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\rb\\Anaconda3\\envs\\geo:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "alembic                   1.4.3              pyh9f0ad1d_0    conda-forge\n",
      "argon2-cffi               20.1.0           py38h294d835_2    conda-forge\n",
      "async_generator           1.10                       py_0    conda-forge\n",
      "attrs                     20.3.0             pyhd3deb0d_0    conda-forge\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\n",
      "backports                 1.0                        py_2    conda-forge\n",
      "backports.functools_lru_cache 1.6.1                      py_0    conda-forge\n",
      "bleach                    3.2.1              pyh9f0ad1d_0    conda-forge\n",
      "blinker                   1.4                        py_1    conda-forge\n",
      "boost-cpp                 1.74.0               h54f0996_1    conda-forge\n",
      "brotlipy                  0.7.0           py38hab1e662_1001    conda-forge\n",
      "bzip2                     1.0.8                he774522_3    conda-forge\n",
      "ca-certificates           2020.12.5            h5b45459_0    conda-forge\n",
      "cairo                     1.16.0            hd28d34b_1006    conda-forge\n",
      "certifi                   2020.12.5        py38haa244fe_1    conda-forge\n",
      "certipy                   0.1.3                      py_0    conda-forge\n",
      "cffi                      1.14.4           py38hd8c33c5_0    conda-forge\n",
      "cfitsio                   3.470                h0af3d06_7    conda-forge\n",
      "cftime                    1.3.1            py38h347fdf6_0    conda-forge\n",
      "chardet                   3.0.4           py38h9bdc248_1008    conda-forge\n",
      "click                     7.1.2              pyh9f0ad1d_0    conda-forge\n",
      "click-plugins             1.1.1                      py_0    conda-forge\n",
      "cligj                     0.7.1              pyhd8ed1ab_0    conda-forge\n",
      "colorama                  0.4.4              pyh9f0ad1d_0    conda-forge\n",
      "configurable-http-proxy   1.3.0                         0    conda-forge\n",
      "cryptography              3.2.1            py38hd8c33c5_0    conda-forge\n",
      "curl                      7.71.1               h4b64cdc_8    conda-forge\n",
      "cycler                    0.10.0                     py_2    conda-forge\n",
      "decorator                 4.4.2                      py_0    conda-forge\n",
      "defusedxml                0.6.0                      py_0    conda-forge\n",
      "entrypoints               0.3             pyhd8ed1ab_1003    conda-forge\n",
      "expat                     2.2.9                h33f27b4_2    conda-forge\n",
      "fiona                     1.8.18           py38h60f4e94_0    conda-forge\n",
      "freetype                  2.10.4               h546665d_0    conda-forge\n",
      "freexl                    1.0.5             hd288d7e_1002    conda-forge\n",
      "gdal                      3.1.4            py38h8f7194f_0    conda-forge\n",
      "geoalchemy2               0.6.3                      py_0    conda-forge\n",
      "geopandas                 0.8.2              pyhd8ed1ab_0    conda-forge\n",
      "geos                      3.8.1                he025d50_0    conda-forge\n",
      "geotiff                   1.6.0                h8884d1a_3    conda-forge\n",
      "gettext                   0.19.8.1          hfbb10ce_1004    conda-forge\n",
      "glib                      2.66.3               h0e60522_0    conda-forge\n",
      "hdf4                      4.2.13            hf8e6fe8_1003    conda-forge\n",
      "hdf5                      1.10.6          nompi_h2a0e4a3_1111    conda-forge\n",
      "icu                       67.1                 h33f27b4_0    conda-forge\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\n",
      "importlib-metadata        3.1.0              pyhd8ed1ab_0    conda-forge\n",
      "importlib_metadata        3.1.0                hd8ed1ab_0    conda-forge\n",
      "intel-openmp              2020.3             h57928b3_311    conda-forge\n",
      "ipykernel                 5.3.4            py38h7b7c402_1    conda-forge\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.ipython                   7.19.0           py38hc5df569_0    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "ipywidgets                7.5.1              pyh9f0ad1d_1    conda-forge\n",
      "jedi                      0.17.2           py38haa244fe_1    conda-forge\n",
      "jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\n",
      "jpeg                      9d                   h8ffe710_0    conda-forge\n",
      "json5                     0.9.5              pyh9f0ad1d_0    conda-forge\n",
      "jsonschema                3.2.0                      py_2    conda-forge\n",
      "jupyter                   1.0.0                      py_2    conda-forge\n",
      "jupyter_client            6.1.7                      py_0    conda-forge\n",
      "jupyter_console           6.2.0                      py_0    conda-forge\n",
      "jupyter_core              4.7.0            py38haa244fe_0    conda-forge\n",
      "jupyter_telemetry         0.1.0              pyhd8ed1ab_1    conda-forge\n",
      "jupyterhub                1.2.1            py38haa244fe_0    conda-forge\n",
      "jupyterhub-base           1.2.1            py38haa244fe_0    conda-forge\n",
      "jupyterlab                2.2.9                      py_0    conda-forge\n",
      "jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\n",
      "jupyterlab_server         1.2.0                      py_0    conda-forge\n",
      "kealib                    1.4.13               h3b59ab9_1    conda-forge\n",
      "\n",
      "kiwisolver                1.3.1            py38hbd9d945_0    conda-forge\n",
      "krb5                      1.17.2               hbae68bd_0    conda-forge\n",
      "libblas                   3.8.0                    21_mkl    conda-forge\n",
      "libcblas                  3.8.0                    21_mkl    conda-forge\n",
      "libclang                  10.0.1          default_hf44288c_1    conda-forge\n",
      "libcurl                   7.71.1               h4b64cdc_8    conda-forge\n",
      "libffi                    3.2.1             ha925a31_1007    conda-forge\n",
      "libgdal                   3.1.4                h0e5aa5a_0    conda-forge\n",
      "libglib                   2.66.3               h35efcdc_0    conda-forge\n",
      "libiconv                  1.16                 he774522_0    conda-forge\n",
      "libkml                    1.3.0             he9e54da_1012    conda-forge\n",
      "liblapack                 3.8.0                    21_mkl    conda-forge\n",
      "libnetcdf                 4.7.4           nompi_h2ee746f_106    conda-forge\n",
      "libpng                    1.6.37               h1d00b33_2    conda-forge\n",
      "libpq                     12.3                 hd9aa61d_2    conda-forge\n",
      "libsodium                 1.0.18               h8d14728_1    conda-forge\n",
      "libspatialindex           1.9.3                he025d50_3    conda-forge\n",
      "libspatialite             5.0.0                hf693123_0    conda-forge\n",
      "libssh2                   1.9.0                hb06d900_5    conda-forge\n",
      "libtiff                   4.1.0                hc10be44_6    conda-forge\n",
      "libwebp-base              1.1.0                h8ffe710_3    conda-forge\n",
      "libxml2                   2.9.10               h1006b36_2    conda-forge\n",
      "lz4-c                     1.9.2                h62dcd97_2    conda-forge\n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "mako                      1.1.3              pyh9f0ad1d_0    conda-forge\n",
      "markupsafe                1.1.1            py38hab1e662_2    conda-forge\n",
      "matplotlib                3.3.3            py38haa244fe_0    conda-forge\n",
      "matplotlib-base           3.3.3            py38h34ddff4_0    conda-forge\n",
      "mistune                   0.8.4           py38h294d835_1002    conda-forge\n",
      "mkl                       2020.4             hb70f87d_311    conda-forge\n",
      "msys2-conda-epoch         20160418                      1  \n",
      "munch                     2.5.0                      py_0    conda-forge\n",
      "nbclient                  0.5.1                      py_0    conda-forge\n",
      "nbconvert                 6.0.7            py38haa244fe_3    conda-forge\n",
      "nbformat                  5.0.8                      py_0    conda-forge\n",
      "nest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge\n",
      "netcdf4                   1.5.5.1         nompi_py38h5338a22_100    conda-forge\n",
      "nodejs                    15.3.0               h57928b3_0    conda-forge\n",
      "notebook                  6.1.5            py38haa244fe_0    conda-forge\n",
      "numpy                     1.19.4           py38h0cc643e_1    conda-forge\n",
      "oauthlib                  3.0.1                      py_0    conda-forge\n",
      "olefile                   0.46               pyh9f0ad1d_1    conda-forge\n",
      "openjpeg                  2.3.1                h57dd2e7_3    conda-forge\n",
      "openssl                   1.1.1i               h8ffe710_0    conda-forge\n",
      "packaging                 20.4               pyh9f0ad1d_0    conda-forge\n",
      "pandas                    1.1.4            py38h4c96930_0    conda-forge\n",
      "pandoc                    2.11.2               h8ffe710_0    conda-forge\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\n",
      "parso                     0.7.1              pyh9f0ad1d_0    conda-forge\n",
      "patsy                     0.5.1                      py_0    conda-forge\n",
      "pcre                      8.44                 ha925a31_0    conda-forge\n",
      "pickleshare               0.7.5                   py_1003    conda-forge\n",
      "pillow                    8.0.1            py38hd8d9125_0    conda-forge\n",
      "pip                       20.2.4                     py_0    conda-forge\n",
      "pixman                    0.40.0               h8ffe710_0    conda-forge\n",
      "poppler                   0.89.0               h5d62644_1    conda-forge\n",
      "poppler-data              0.4.10                        0    conda-forge\n",
      "postgresql                12.3                 he14cc48_2    conda-forge\n",
      "proj                      7.1.1                h7d85306_3    conda-forge\n",
      "prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge\n",
      "prompt-toolkit            3.0.8              pyha770c72_0    conda-forge\n",
      "prompt_toolkit            3.0.8                hd8ed1ab_0    conda-forge\n",
      "psutil                    5.7.3            py38hab1e662_0    conda-forge\n",
      "psycopg2                  2.8.6            py38hd8c33c5_1    conda-forge\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\n",
      "pycurl                    7.43.0.6         py38h6be4be5_0    conda-forge\n",
      "pygments                  2.7.2                      py_0    conda-forge\n",
      "pyjwt                     1.7.1                      py_0    conda-forge\n",
      "pyopenssl                 19.1.0                     py_1    conda-forge\n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\n",
      "pyproj                    2.6.1.post1      py38hbdc76b6_3    conda-forge\n",
      "pyqt                      5.12.3           py38h7ae7562_4    conda-forge\n",
      "pyqt5-sip                 4.19.18                  pypi_0    pypi\n",
      "pyqtchart                 5.12                     pypi_0    pypi\n",
      "pyqtwebengine             5.12.1                   pypi_0    pypi\n",
      "pyrsistent                0.17.3           py38h294d835_1    conda-forge\n",
      "pysocks                   1.7.1            py38h9bdc248_2    conda-forge\n",
      "python                    3.8.6           h60c2a47_0_cpython    conda-forge\n",
      "python-dateutil           2.8.1                      py_0    conda-forge\n",
      "python-editor             1.0.4                      py_0    conda-forge\n",
      "python-json-logger        2.0.1              pyh9f0ad1d_0    conda-forge\n",
      "python_abi                3.8                      1_cp38    conda-forge\n",
      "pytz                      2020.4             pyhd8ed1ab_0    conda-forge\n",
      "pywin32                   228              py38h1e8a9f7_0    conda-forge\n",
      "pywinpty                  0.5.7            py38h32f6830_1    conda-forge\n",
      "pyzmq                     20.0.0           py38h7a0e47e_1    conda-forge\n",
      "qt                        5.12.9               hb2cf2c5_0    conda-forge\n",
      "qtconsole                 5.0.0              pyhd8ed1ab_0    conda-forge\n",
      "qtpy                      1.9.0                      py_0    conda-forge\n",
      "requests                  2.25.0             pyhd3deb0d_0    conda-forge\n",
      "rtree                     0.9.7            py38h8b54edf_1    conda-forge\n",
      "ruamel.yaml               0.16.12          py38h294d835_1    conda-forge\n",
      "ruamel.yaml.clib          0.2.2            py38h294d835_1    conda-forge\n",
      "scipy                     1.5.3            py38h5f893b4_0    conda-forge\n",
      "seaborn                   0.11.0               h57928b3_1    conda-forge\n",
      "seaborn-base              0.11.0             pyhd8ed1ab_1    conda-forge\n",
      "send2trash                1.5.0                      py_0    conda-forge\n",
      "setuptools                49.6.0           py38h9bdc248_2    conda-forge\n",
      "shapely                   1.7.1            py38h6953702_1    conda-forge\n",
      "six                       1.15.0             pyh9f0ad1d_0    conda-forge\n",
      "sqlalchemy                1.3.20           py38h1e8a9f7_0    conda-forge\n",
      "sqlite                    3.33.0               he774522_1    conda-forge\n",
      "statsmodels               0.12.1           py38h347fdf6_1    conda-forge\n",
      "terminado                 0.9.1            py38haa244fe_1    conda-forge\n",
      "testpath                  0.4.4                      py_0    conda-forge\n",
      "tiledb                    2.1.3                h968eb34_0    conda-forge\n",
      "tk                        8.6.10               he774522_1    conda-forge\n",
      "tornado                   6.1              py38h294d835_0    conda-forge\n",
      "traitlets                 5.0.5                      py_0    conda-forge\n",
      "urllib3                   1.25.11                    py_0    conda-forge\n",
      "vc                        14.1                 h869be7e_1    conda-forge\n",
      "vs2015_runtime            14.16.27012          h30e32a0_2    conda-forge\n",
      "wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\n",
      "webencodings              0.5.1                      py_1    conda-forge\n",
      "wheel                     0.35.1             pyh9f0ad1d_0    conda-forge\n",
      "widgetsnbextension        3.5.1            py38haa244fe_4    conda-forge\n",
      "win_inet_pton             1.1.0            py38h32f6830_1    conda-forge\n",
      "wincertstore              0.2             py38h32f6830_1005    conda-forge\n",
      "winpty                    0.4.3                         4    conda-forge\n",
      "xerces-c                  3.2.3                ha925a31_1    conda-forge\n",
      "xz                        5.2.5                h62dcd97_1    conda-forge\n",
      "zeromq                    4.3.3                h0e60522_3    conda-forge\n",
      "zipp                      3.4.0                      py_0    conda-forge\n",
      "zlib                      1.2.11            h62dcd97_1010    conda-forge\n",
      "zstd                      1.4.5                h1f3a1b7_2    conda-forge\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
